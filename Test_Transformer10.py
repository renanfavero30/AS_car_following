# -*- coding: utf-8 -*-
"""
Created on Tue Mar  4 20:01:52 2025
@author: renanfavero
"""

import os
import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from tensorflow.keras.models import load_model
import numpy as np
import os








def load_transformer_model(results_folder):
    """
    Loads the trained Transformer model and the scaling parameters (mean/std).
    
    Parameters:
    - results_folder: Path where the trained model is saved.
    
    Returns:
    - model: Loaded trained Transformer model
    - X_mean: Feature means for standardization
    - X_std: Feature standard deviations for standardization
    - y_mean: Target mean for inverse transformation
    - y_std: Target standard deviation for inverse transformation
    """
    model_filename = os.path.join(results_folder, "final_transformer_model.h5")
    model = load_model(model_filename)
    print(f"‚úÖ Transformer Model loaded from: {model_filename}")

    # Load stored means & stds from training (ensure they are saved properly)
    X_mean = np.load(os.path.join(results_folder, "X_mean.npy"))
    X_std = np.load(os.path.join(results_folder, "X_std.npy"))
    y_mean = np.load(os.path.join(results_folder, "y_mean.npy"))
    y_std = np.load(os.path.join(results_folder, "y_std.npy"))

    print(f"‚úÖ Loaded feature scaling parameters.")

    return model, X_mean, X_std, y_mean, y_std



# ‚úÖ Load & Preprocess Test Data
def load_test_data(test_file_name, variables_used):
    """
    Loads and preprocesses the test dataset, ensuring feature consistency.

    Parameters:
    - test_file_name: Path to the test dataset CSV file.
    - variables_used: List of feature names used in training.

    Returns:
    - df_test: Processed test DataFrame
    - x_test: Features matrix (unscaled)
    - y_test: Target values (unscaled)
    """
    df_test = pd.read_csv(test_file_name)
    print(f"‚úÖ Test dataset loaded from: {test_file_name}")

    # ‚úÖ Apply preprocessing: Feature selection, column renaming, and target alignment
    X_test = df_test.drop(columns=["Follower_acc"]).copy()
    X_test.columns = X_test.columns.str.replace(r"[\[\]]", "", regex=True)
    X_test = X_test.loc[:, X_test.columns.isin(variables_used)]

    # ‚úÖ Ensure correct feature order before transformation
    expected_feature_order = [col for col in variables_used if col in X_test.columns]
    X_test = X_test[expected_feature_order]

    y_test = df_test["Follower_acc"].values.reshape(-1, 1)  # Convert y_test to NumPy array

    print("‚úÖ Final features used for testing:", list(X_test.columns))
    return df_test, X_test, y_test


# ‚úÖ Create Sequences (Only for CNN, LSTM, Transformer)
def create_sequences(X_scaled, y_scaled, input_size, horizon):
    """
    Creates sequences ensuring the predicted `y` aligns with `y_true` at `t+horizon`.
    X[t-4] ... X[t]  ---> Predict y[t+1] (for horizon=1)
    """
    X_seq, y_seq = [], []
    for i in range(len(X_scaled) - input_size - horizon + 1):
        X_seq.append(X_scaled[i:i + input_size])
        y_seq.append(y_scaled[i + input_size + horizon - 1])

    return np.array(X_seq), np.array(y_seq).reshape(-1, 1)


# ‚úÖ Make Predictions
def make_predictions(model, x_test, y_test, scaler_x, scaler_y, model_type, input_size=5, horizon=1):
    """
    Makes predictions using the trained model, ensuring input shape matches training.

    Parameters:
    - model: Trained ML model
    - x_test: Test dataset features (unscaled)
    - y_test: Target values (unscaled)
    - scaler_x: Scaler used for feature normalization
    - scaler_y: Scaler used for target normalization
    - model_type: Model type (CNN, LSTM, Transformer, etc.)
    - input_size: Number of past time steps used for training sequences (if applicable)
    - horizon: Prediction horizon (default=1 step ahead)

    Returns:
    - y_pred_unscaled: Unscaled predictions
    """

    # ‚úÖ Ensure correct feature order
    expected_feature_order = scaler_x.feature_names_in_
    x_test = x_test[expected_feature_order]

    # ‚úÖ Normalize test data
    x_test_scaled = scaler_x.transform(x_test)
    y_test_scaled = scaler_y.transform(y_test) if scaler_y else y_test

    # ‚úÖ Convert test data into sequences (only for time-series models)
    if model_type in ["CNN", "LSTM", "Transformer"]:
        x_test_scaled, _ = create_sequences(x_test_scaled, y_test_scaled, input_size, horizon)

    #print(f"‚úÖ Reshaped x_test for prediction: {x_test_scaled.shape}")

    # ‚úÖ Make predictions
    y_pred_scaled = model.predict(x_test_scaled)

    # ‚úÖ Unscale predictions
    y_pred_unscaled = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)) if scaler_y else y_pred_scaled

    return y_pred_unscaled, x_test_scaled, y_test_scaled

def make_transformer_predictions(model, x_test, y_test, X_mean, X_std, y_mean, y_std, input_size=5, horizon=1):
    """
    Makes predictions using the trained Transformer model, ensuring input shape matches training.

    Parameters:
    - model: Trained Transformer model
    - x_test: Test dataset features (unscaled)
    - y_test: Target values (unscaled)
    - X_mean, X_std: Feature mean and standard deviation from training
    - y_mean, y_std: Target mean and standard deviation from training
    - input_size: Number of past time steps used for sequences
    - horizon: Prediction horizon (default=1 step ahead)

    Returns:
    - y_pred_unscaled: Unscaled predictions
    """

    # ‚úÖ Standardize test data manually
    x_test_scaled = (x_test - X_mean) / X_std
    y_test_scaled = (y_test - y_mean) / y_std  # Standardize target as well

    # ‚úÖ Convert test data into sequences
    x_test_scaled_seq, _ = create_sequences(x_test_scaled, y_test_scaled, input_size, horizon)

    #print(f"‚úÖ Reshaped x_test for prediction: {x_test_scaled_seq.shape}")

    # ‚úÖ Make predictions
    y_pred_scaled = model.predict(x_test_scaled_seq)

    # ‚úÖ Reverse standardization for predictions
    y_pred_unscaled = (y_pred_scaled * y_std) + y_mean

    return y_pred_unscaled, x_test_scaled_seq, y_test_scaled




def save_predictions(df_test, y_pred_unscaled, results_folder, model_type, input_size=5):
    """
    Saves the predicted and true values in a CSV file.

    Parameters:
    - df_test: Test dataset DataFrame
    - y_pred_unscaled: Unscaled predicted values
    - results_folder: Path to save results
    - model_type: Type of model (CNN, Transformer, LSTM, RF, LGBM, etc.)
    - input_size: Number of past time steps used in sequence models (default = 10)
    """

    # ‚úÖ Check if model requires sequence trimming
    sequence_models = ["CNN", "Transformer", "LSTM"]

    if model_type in sequence_models:
        df_test_trimmed = df_test.iloc[input_size:].reset_index(drop=True)  # Trim first `input_size` rows
        #print(f"‚úÖ Applied sequence trimming for {model_type}: Removed first {input_size} rows.")
    else:
        df_test_trimmed = df_test.copy()  # No trimming needed for RF, LGBM, etc.

    # ‚úÖ Create DataFrame with predictions
    predictions_df = pd.DataFrame({
        "Time_[s]": df_test_trimmed["Time_[s]"],
        "True Acceleration": df_test_trimmed["Follower_acc"],
        "Predicted Acceleration": y_pred_unscaled.flatten()
    })

    # ‚úÖ Save predictions to CSV
    predictions_file = os.path.join(results_folder, f"predictions_test_{model_type}.csv")
    predictions_df.to_csv(predictions_file, index=False)
    print(f"‚úÖ Predictions saved at: {predictions_file}")




def simulate_vehicle_motion(df_test, model, results_folder, X_mean, X_std, y_mean, y_std, model_type, x_test_scaled_seq, y_test_scaled, input_size, horizon =1):
    """
    Simulates vehicle motion using the trained model by iteratively predicting acceleration,
    updating speed, and computing position with realistic constraints.

    Returns:
    - df_simulated: DataFrame containing simulated features, speed, and position.
    """
    print(f"üöÄ Starting Simulation for {model_type}")

    # ‚úÖ Initialize dataset
    df_simulated = df_test.copy()
    
    is_sequential = model_type.lower() in ["lstm", "gru","cnn","transformer"]
    # ‚úÖ Define all simulated columns
    simulated_columns = {
        "Simulated_acc": "Follower_acc",
        "Simulated_sp": "Follower_sp_[ft]",
        "Simulated_pos": "Follower_pos_[ft]",
        "Simulated_delta_s": "delta_s",
        "Simulated_delta_v": "delta_v",
        "Simulated_Follower_acc_t-1": "Follower_acc_t-1"
    }
    
    # ‚úÖ Initialize simulated columns
    for sim_col, orig_col in simulated_columns.items():
        df_simulated[f"{sim_col}_{model_type}"] = df_simulated[orig_col]

    # ‚úÖ Define feature columns dynamically based on model type
    if is_sequential:
        feature_columns = [f"Simulated_delta_s_{model_type}", f"Simulated_delta_v_{model_type}",
                           f"Simulated_sp_{model_type}", f"Simulated_Follower_acc_t-1_{model_type}"]
    else:
        feature_columns = list(simulated_columns.values())  # Use original column names for non-sequential models


    # ‚úÖ Compute time step differences
    dt = df_simulated["Time_[s]"].diff().fillna(1).values

    # ‚úÖ Define speed constraints
    max_speed = 22  # Maximum allowable speed
    min_speed = 0   # Minimum allowable speed (no reverse)

    # ‚úÖ Define input features for model
    #feature_columns = [f"Simulated_delta_s_{model_type}", f"Simulated_delta_v_{model_type}",
                       #f"Simulated_sp_{model_type}", f"Simulated_Follower_acc_t-1_{model_type}"]
                       

    # Define the mapping between simulated and original feature names
    feature_mapping = {
        f"Simulated_delta_s_{model_type}": "delta_s",
        f"Simulated_delta_v_{model_type}": "delta_v",
        f"Simulated_sp_{model_type}": "Follower_sp_ft",
        f"Simulated_Follower_acc_t-1_{model_type}": "Follower_acc_t-1",
    }
    

    # ‚úÖ Simulation Loop
    for t in range(input_size, len(df_simulated) - horizon):  
        # Detect trajectory resets when delta_t > 2 or delta_t < 1
        if df_simulated.at[t, "delta_t"] > 1 or df_simulated.at[t, "delta_t"] < 1:  
            print(f"üîÑ Resetting simulation at t={t}, new trajectory detected (delta_t={df_simulated.at[t, 'delta_t']}).")
            
            # ‚úÖ Copy original values or use default (0.5 if missing)
            for sim_col, orig_col in simulated_columns.items():
                df_simulated.at[t, f"{sim_col}_{model_type}"] = (
                    df_simulated.at[t, orig_col] if pd.notna(df_simulated.at[t, orig_col]) else 0.5
                )
    
            continue  # Skip update since we initialize with real-world data

        # ‚úÖ Prepare input for prediction
        # ‚úÖ Prepare input for prediction
        if is_sequential:
            # Sequential model (Transformer) - Use past `input_size` steps as input
            past_window = []
        
            #print("Feature Columns:", feature_columns)
            #print("X_mean Keys:", list(X_mean.keys()))
            #print("X_std Keys:", list(X_std.keys()))
        
            for i in range(t - input_size, t):
                row = []
                for col in feature_columns:
                    # Map the simulated column name to the original feature name
                    original_col = feature_mapping.get(col, None)
        
                    if original_col is None:
                        print(f"‚ö†Ô∏è WARNING: Column {col} is missing in feature_mapping. Using 0 as default.")
                        normalized_value = 0  # Default to 0 if column mapping is missing
                    elif original_col not in X_mean or original_col not in X_std:
                        print(f"‚ö†Ô∏è WARNING: Column {col} (mapped to {original_col}) not found in X_mean/X_std. Using 0 as default.")
                        normalized_value = 0  # Default to 0 if missing in X_mean/X_std
                    else:
                        # Normalize value using mean and std
                        raw_value = df_simulated.at[i, col]
                        if pd.isna(raw_value):  # Handle NaN values explicitly
                            print(f"‚ö†Ô∏è WARNING: NaN detected for {col} at index {i}. Using 0 as default.")
                            normalized_value = 0
                        else:
                            normalized_value = (raw_value - X_mean[original_col]) / X_std[original_col]
        
                    row.append(normalized_value)
        
                #print(f"üü¢ Processed row {i}: {row}")
                past_window.append(row)
        
            # Convert past_window to NumPy array with correct shape
            x_input = np.array(past_window).reshape(1, input_size, len(feature_columns))
        
            #print(f"‚úÖ x_input shape: {x_input.shape}")

            # Non-sequential model (XGBoost, Random Forest) - Use latest simulated values
        if not is_sequential:
            temp_input = df_simulated.loc[t, [f"Simulated_{col}" for col in feature_columns]].copy()
            temp_input.columns = feature_columns  # Temporarily rename for model compatibility
            x_input = temp_input.values.reshape(1, -1)

    
        # ‚úÖ Debug: Check Input Shape Before Prediction
        #print(f"üü¢ Predicting at t={t}: x_input.shape = {x_input.shape}")
    
        # ‚úÖ Make Prediction
        try:
            #print(f"Expected model input shape: {model.input_shape}")
            #print(f"Actual x_input shape: {x_input.shape}")
            #print(x_input)
            y_pred_scaled = model.predict(x_input)
            #print(y_pred_scaled)
            y_pred = (y_pred_scaled * y_std) + y_mean  # Unscale prediction
            y_pred = np.clip(y_pred, -10, 10)  # Limit acceleration between -10 and 10
        except Exception as e:
            print(f"‚ùå Prediction failed at t={t}: {e}")
            continue
    
        # ‚úÖ Get predicted acceleration
        pred_acc = y_pred.flatten()[0]
        print(y_pred)
    
        # ‚úÖ Get current speed
        current_speed = df_simulated.at[t, f"Simulated_sp_{model_type}"]
        
        # ‚úÖ If speed is already at max, ensure acceleration does not increase it further
        if current_speed >= max_speed and pred_acc > 0:
            pred_acc = 0  # Prevent further acceleration if already at max speed
        # Ensure acceleration does not reverse the vehicle
        if current_speed <= min_speed and pred_acc < 0:
            pred_acc = 0  # Prevent negative acceleration from reversing the vehicle
    
        # ‚úÖ Store the corrected acceleration
        df_simulated.at[t + horizon, f"Simulated_acc_{model_type}"] = pred_acc


        # ‚úÖ Compute New Speed with Constraints
        dt_t = dt[t + horizon] if t + horizon < len(dt) else dt[-1]  # Ensure valid index
        new_speed = df_simulated.at[t, f"Simulated_sp_{model_type}"] + pred_acc * dt_t
        # üö® Apply Speed Constraints
        new_speed = max(0, min(new_speed, max_speed))  # Ensure speed never goes negative


        # ‚úÖ Store the Corrected Speed
        df_simulated.at[t + horizon, f"Simulated_sp_{model_type}"] = new_speed

        # ‚úÖ Compute New Position
        df_simulated.at[t + horizon, f"Simulated_pos_{model_type}"] = (
            df_simulated.at[t, f"Simulated_pos_{model_type}"] +
            new_speed * dt_t +  # Use corrected speed
            0.5 * pred_acc * (dt_t ** 2)
        )

        # ‚úÖ Update Simulated Feature Columns for Next Step **Before Prediction**
        # ‚úÖ Update delta_s and delta_v using the latest simulated values
        # ‚úÖ Store the last predicted acceleration as Follower_acc_t-1 for the next step
        df_simulated.at[t + horizon, f"Simulated_Follower_acc_t-1_{model_type}"] = pred_acc
        leader_position = df_simulated.at[t + horizon, "Leader_pos_[ft]"] if (t + horizon) < len(df_simulated) else df_simulated.at[t, "Leader_pos_[ft]"]
        leader_speed = df_simulated.at[t + horizon, "Leader_sp_[ft]"] if (t + horizon) < len(df_simulated) else df_simulated.at[t, "Leader_sp_[ft]"]
        follower_position = df_simulated.at[t + horizon, f"Simulated_pos_{model_type}"]

        df_simulated.at[t + horizon , f"Simulated_delta_s_{model_type}"] = leader_position - follower_position
        df_simulated.at[t + horizon, f"Simulated_delta_v_{model_type}"] = leader_speed - new_speed

        # ‚úÖ Debugging Outputs for Verification
        #print(f"‚úÖ Updated at t={t + horizon}: acc={pred_acc:.4f}, sp={new_speed:.4f}, pos={df_simulated.at[t + horizon, f'Simulated_pos_{model_type}']:.4f}, delta_s={df_simulated.at[t + horizon, f'Simulated_delta_s_{model_type}']:.4f}, delta_v={df_simulated.at[t + horizon, f'Simulated_delta_v_{model_type}']:.4f}")

    # ‚úÖ Save the updated simulated dataset
    simulated_file = os.path.join(r"C:\Users\renanfavero\OneDrive - University of Florida\My_research\Lake_Nona_2022\CSV_2", "dataset_test_simulation.csv")
    df_simulated.to_csv(simulated_file, index=False)
        #print(f"‚úÖ Simulated results saved at: {simulated_file}")

    return df_simulated


def evaluate_model(df_test, y_pred_unscaled, results_folder, model_type, input_size=5):
    """
    Computes regression metrics and ensures consistent trimming for models that use sequences.

    Parameters:
    - df_test: Test dataset DataFrame
    - y_pred_unscaled: Unscaled predicted values
    - results_folder: Path to save results
    - model_type: Type of model (CNN, Transformer, LSTM, RF, LGBM, etc.)
    - input_size: Number of past time steps used in sequence models (default = 10)

    Returns:
    - df_test_trimmed: Trimmed test dataset
    - y_pred_trimmed: Trimmed predicted values
    """

    print(f"üìè Original df_test length: {len(df_test)}, y_pred_unscaled length: {len(y_pred_unscaled)}")

    # ‚úÖ Trim first `input_size` rows only for sequence-based models
    sequence_models = ["CNN", "Transformer", "LSTM"]
    if model_type in sequence_models:
        df_test_trimmed = df_test.drop(index=range(input_size)).reset_index(drop=True)
        y_pred_trimmed = y_pred_unscaled # Trim predictions to match

        print(f"‚úÖ Trimmed first {input_size} rows for {model_type} -> New df_test length: {len(df_test_trimmed)}")
    else:
        df_test_trimmed = df_test.copy()  # No trimming needed for RF, LGBM, etc.
        y_pred_trimmed = y_pred_unscaled.copy()

    # ‚úÖ Check for NaNs
    print(f"üîç NaN Check in df_test_trimmed: {df_test_trimmed.isna().sum().sum()} NaN values")
    print(f"üîç NaN Check in y_pred_trimmed: {np.isnan(y_pred_trimmed).sum()} NaN values")

    if df_test_trimmed.isna().sum().sum() > 0:
        print("‚ö†Ô∏è Replacing NaNs in df_test_trimmed with column means.")
        df_test_trimmed.fillna(df_test_trimmed.mean(), inplace=True)

    if np.isnan(y_pred_trimmed).sum() > 0:
        print("‚ö†Ô∏è Replacing NaNs in y_pred_trimmed with mean prediction.")
        y_pred_trimmed = np.nan_to_num(y_pred_trimmed, nan=np.nanmean(y_pred_trimmed))

    # ‚úÖ Ensure final lengths match exactly
    if len(df_test_trimmed) != len(y_pred_trimmed):
        raise ValueError(f"‚ùå Length mismatch after trimming: df_test_trimmed has {len(df_test_trimmed)} rows, "
                         f"but y_pred_trimmed has {len(y_pred_trimmed)}.")
    
    # ‚úÖ Compute regression metrics
    mse = mean_squared_error(df_test_trimmed["Follower_acc"], y_pred_trimmed)
    mae = mean_absolute_error(df_test_trimmed["Follower_acc"], y_pred_trimmed)
    r2 = r2_score(df_test_trimmed["Follower_acc"], y_pred_trimmed)
    rmse = np.sqrt(mse)
    
    # ‚úÖ Compute Normalized RMSE (NRMSE)
    nrmse = rmse / (df_test_trimmed["Follower_acc"].max() - df_test_trimmed["Follower_acc"].min())
    
    # ‚úÖ Store metrics in a dictionary
    metrics = {
        "MSE": mse,
        "MAE": mae,
        "R2 Score": r2,
        "RMSE": rmse,
        "NRMSE": nrmse
    }
    
    # ‚úÖ Save metrics to file efficiently
    metrics_file = os.path.join(results_folder, f"metrics_test_{model_type}.txt")
    with open(metrics_file, "w") as f:
        f.write(f"Model Type: {model_type}\n")
        f.write("\n".join(f"{key}: {value:.6f}" for key, value in metrics.items()) + "\n")
    
    # ‚úÖ Print metrics to console
    print("\nüìä Predictions Metrics:")
    print("\n".join(f"{key}: {value:.6f}" for key, value in metrics.items()))


    print(f"‚úÖ Test dataset metrics saved at: {metrics_file}")
    print(f"üìè Final df_test_trimmed length: {len(df_test_trimmed)}, y_pred_trimmed length: {len(y_pred_trimmed)}")
    
    # ‚úÖ Create a new DataFrame with aligned predictions
    acc_column_name = f'acc_pred_{model_type} '
    df_test_trimmed[acc_column_name] = y_pred_trimmed.flatten()
    
   
    # ‚úÖ Save the new DataFrame
    combined_file = os.path.join(results_folder, f"test_with_predictions.csv")
    df_test_trimmed.to_csv(combined_file, index=False)
    print(f"‚úÖ New test dataset with predictions saved at: {combined_file}")


    return df_test_trimmed, y_pred_trimmed, combined_file


def plot_results(df_test, y_pred_unscaled, results_folder, model_type, input_size=5):
    """
    Plots and saves the Real vs. Predicted Acceleration and Histogram.

    Parameters:
    - df_test: Test dataset DataFrame
    - y_pred_unscaled: Unscaled predicted values
    - results_folder: Path to save results
    - model_type: Type of model (CNN, Transformer, LSTM, RF, LGBM, etc.)
    - input_size: Number of past time steps used in sequence models (default = 10)
    """

    print(f"üìè Initial df_test length: {len(df_test)}, y_pred_unscaled length: {len(y_pred_unscaled)}")
    
    # ‚úÖ Ensure `results_folder` is a valid string
    if not isinstance(results_folder, str):
        results_folder = str(results_folder)  # Convert to string
        print(f"‚ö†Ô∏è Warning: `results_folder` was not a string, converted to: {results_folder}")

    # ‚úÖ Ensure sequence-based models trim the first `input_size` rows
    sequence_models = ["CNN", "Transformer", "LSTM"]
    if model_type in sequence_models:
        #df_test_trimmed = df_test.drop(index=range(input_size)).reset_index(drop=True)
        df_test_trimmed = df_test.reset_index(drop=True)
        #y_pred_trimmed = y_pred_unscaled[input_size:].copy()
        y_pred_trimmed = y_pred_unscaled[:].copy()

        print(f"‚úÖ Trimmed first {input_size} rows for {model_type} -> New df_test length: {len(df_test_trimmed)}, and y_pred_trimmed =  {len(y_pred_trimmed)} ")
    else:
        df_test_trimmed = df_test.copy()
        y_pred_trimmed = y_pred_unscaled.copy()

    # ‚úÖ Ensure no NaNs before plotting
    if df_test_trimmed.isna().sum().sum() > 0:
        print("‚ö†Ô∏è Replacing NaNs in df_test_trimmed with column means.")
        df_test_trimmed.fillna(df_test_trimmed.mean(), inplace=True)

    if np.isnan(y_pred_trimmed).sum() > 0:
        print("‚ö†Ô∏è Replacing NaNs in y_pred_trimmed with mean prediction.")
        y_pred_trimmed = np.nan_to_num(y_pred_trimmed, nan=np.nanmean(y_pred_trimmed))

    # ‚úÖ Ensure final lengths match before plotting
    if len(df_test_trimmed) != len(y_pred_trimmed):
        raise ValueError(f"‚ùå Length mismatch after trimming: df_test_trimmed has {len(df_test_trimmed)} rows, "
                         f"but y_pred_trimmed has {len(y_pred_trimmed)}.")

    # ‚úÖ Define x-axis as a continuous sequence instead of `Time_[s]`
    x_axis = np.arange(len(df_test_trimmed))

    # ‚úÖ Real vs. Predicted Acceleration Plot
    plt.figure(figsize=(12, 6))
    plt.plot(x_axis, df_test_trimmed["Follower_acc"], label="True Acceleration", alpha=1)
    plt.plot(x_axis, y_pred_trimmed, label=f"Predicted Acceleration ({model_type})", linestyle="--", alpha=1)
    plt.legend()
    plt.xlabel("Sample Index")
    plt.ylabel("Acceleration (ft/s¬≤)")
    plt.title(f"Comparison of Real vs Predicted Accelerations ({model_type})")
    plt.savefig(os.path.join(results_folder, f"real_vs_pred_test_{model_type}.png"))
    plt.show()  # ‚úÖ Display the plot
    print(f"‚úÖ Real vs. Predicted plot saved.")

    # ‚úÖ Histogram of True vs. Predicted Acceleration
    plt.figure(figsize=(10, 5))
    plt.hist(df_test_trimmed["Follower_acc"], bins=30, alpha=0.5, label="True Acceleration")
    plt.hist(y_pred_trimmed, bins=30, alpha=0.5, label=f"Predicted Acceleration ({model_type})")
    plt.legend()
    plt.xlabel("Acceleration (ft/s¬≤)")
    plt.ylabel("Frequency")
    plt.title(f"Histogram of Real vs Predicted Acceleration ({model_type})")
    plt.savefig(os.path.join(results_folder, f"hist_real_vs_pred_test_{model_type}.png"))
    plt.show()  # ‚úÖ Display the plot
    print(f"‚úÖ Histogram plot saved.")

def plot_simulation_results(df_simulated, df_test, results_folder, model_type):
    """
    Plots and saves the comparison of Simulated vs. True values for Acceleration, Speed, and Position.

    Parameters:
    - df_simulated: DataFrame containing simulated acceleration, speed, and position.
    - df_test: Original test dataset DataFrame for comparison.
    - results_folder: Path to save results.
    - model_type: Type of model (CNN, Transformer, LSTM, RF, etc.).
    """

    print(f"üìè Initial df_simulated length: {len(df_simulated)}, df_test length: {len(df_test)}")



    # ‚úÖ Ensure `results_folder` is a valid string
    if not isinstance(results_folder, str):
        results_folder = str(results_folder)
        print(f"‚ö†Ô∏è Warning: `results_folder` was not a string, converted to: {results_folder}")

    # ‚úÖ Ensure no NaNs before plotting
    df_simulated.fillna(df_simulated.mean(), inplace=True)
    df_test.fillna(df_test.mean(), inplace=True)

    # ‚úÖ Define x-axis as a continuous sequence instead of `Time_[s]`
    x_axis = np.arange(len(df_simulated))

    # ‚úÖ Define simulated and true column names
    simulated_acc_col = f"Simulated_acc_{model_type}"
    simulated_sp_col = f"Simulated_sp_{model_type}"
    simulated_pos_col = f"Simulated_pos_{model_type}"

    true_acc_col = "Follower_acc"
    true_sp_col = "Follower_sp_[ft]"
    true_pos_col = "Follower_pos_[ft]"

    # üöÄ **Plot 1: Simulated vs. True Acceleration**
    plt.figure(figsize=(12, 6))
    plt.plot(x_axis, df_test[true_acc_col], label="True Acceleration", alpha=1)
    plt.plot(x_axis, df_simulated[simulated_acc_col], label=f"Simulated Acceleration ({model_type})", linestyle="--", alpha=1)
    plt.legend()
    plt.xlabel("Sample Index")
    plt.ylabel("Acceleration (ft/s¬≤)")
    plt.title(f"Comparison of Simulated vs. True Acceleration ({model_type})")
    plt.savefig(os.path.join(results_folder, f"simulated_vs_true_acc_{model_type}.png"))
    plt.show()
    print(f"‚úÖ Acceleration plot saved.")

    # üöÄ **Plot 2: Simulated vs. True Speed**
    plt.figure(figsize=(12, 6))
    plt.plot(x_axis, df_test[true_sp_col], label="True Speed", alpha=1)
    plt.plot(x_axis, df_simulated[simulated_sp_col], label=f"Simulated Speed ({model_type})", linestyle="--", alpha=1)
    plt.legend()
    plt.xlabel("Sample Index")
    plt.ylabel("Speed (ft/s)")
    plt.title(f"Comparison of Simulated vs. True Speed ({model_type})")
    plt.savefig(os.path.join(results_folder, f"simulated_vs_true_sp_{model_type}.png"))
    plt.show()
    print(f"‚úÖ Speed plot saved.")

    # üöÄ **Plot 3: Simulated vs. True Position**
    plt.figure(figsize=(12, 6))
    plt.plot(x_axis, df_test[true_pos_col], label="True Position", alpha=1)
    plt.plot(x_axis, df_simulated[simulated_pos_col], label=f"Simulated Position ({model_type})", linestyle="--", alpha=1)
    plt.legend()
    plt.xlabel("Sample Index")
    plt.ylabel("Position (ft)")
    plt.title(f"Comparison of Simulated vs. True Position ({model_type})")
    plt.savefig(os.path.join(results_folder, f"simulated_vs_true_pos_{model_type}.png"))
    plt.show()
    print(f"‚úÖ Position plot saved.")


def evaluate_simulation(df_simulated, df_test, results_folder, model_type, input_size=5):
    """
    Computes regression metrics for Simulated vs. True values for Acceleration, Speed, and Position.

    Parameters:
    - df_simulated: DataFrame containing simulated acceleration, speed, and position.
    - df_test: Original test dataset DataFrame for comparison.
    - results_folder: Path to save results.
    - model_type: Type of model (CNN, Transformer, LSTM, RF, etc.).
    - input_size: Number of past time steps used in sequence models (default = 10).

    Returns:
    - df_test_trimmed: Trimmed test dataset
    - df_simulated_trimmed: Trimmed simulated dataset
    - metrics_file: Path to saved metrics file
    """

    print(f"üìè Initial df_simulated length: {len(df_simulated)}, df_test length: {len(df_test)}")

    # ‚úÖ Ensure `results_folder` is a valid string
    if not isinstance(results_folder, str):
        results_folder = str(results_folder)
        print(f"‚ö†Ô∏è Warning: `results_folder` was not a string, converted to: {results_folder}")

    # ‚úÖ Trim first `input_size` rows only for sequence-based models
    sequence_models = ["CNN", "Transformer", "LSTM"]
    if model_type in sequence_models:
        df_simulated_trimmed = df_simulated.iloc[input_size:].reset_index(drop=True)
        df_test_trimmed = df_test.iloc[input_size:].reset_index(drop=True)
        print(f"‚úÖ Trimmed first {input_size} rows for {model_type}")
    else:
        df_simulated_trimmed = df_simulated.copy()
        df_test_trimmed = df_test.copy()

    # ‚úÖ Handle NaNs
    df_simulated_trimmed.fillna(df_simulated_trimmed.mean(), inplace=True)
    df_test_trimmed.fillna(df_test_trimmed.mean(), inplace=True)

    # ‚úÖ Define column names for true vs. simulated values
    true_columns = {
        "Acceleration": "Follower_acc",
        "Speed": "Follower_sp_[ft]",
        "Position": "Follower_pos_[ft]"
    }

    simulated_columns = {
        "Acceleration": f"Simulated_acc_{model_type}",
        "Speed": f"Simulated_sp_{model_type}",
        "Position": f"Simulated_pos_{model_type}"
    }

    # ‚úÖ Compute regression metrics for each variable
    metrics = {}
    for metric_name in ["Acceleration", "Speed", "Position"]:
        y_true = df_test_trimmed[true_columns[metric_name]]
        y_simulated = df_simulated_trimmed[simulated_columns[metric_name]]

        mse = mean_squared_error(y_true, y_simulated)
        mae = mean_absolute_error(y_true, y_simulated)
        r2 = r2_score(y_true, y_simulated)
        rmse = np.sqrt(mse)
        nrmse = rmse / (y_true.max() - y_true.min())

        

        metrics[metric_name] = {"MSE": mse, "MAE": mae, "R2": r2, "RMSE": rmse, "NRMSE": nrmse}

    # ‚úÖ Save metrics to a text file
    metrics_file = os.path.join(results_folder, f"simulation_metrics_{model_type}.txt")
    with open(metrics_file, "w") as f:
        f.write(f"Model Type: {model_type}\n\n")
        for metric_name, values in metrics.items():
            f.write(f"Mean Squared Error (MSE): {values['MSE']:.6f}\n")
            f.write(f"Mean Absolute Error (MAE): {values['MAE']:.6f}\n")
            f.write(f"R2 Score: {values['R2']:.6f}\n")
            f.write(f"Root Mean Squared Error (RMSE): {values['RMSE']:.6f}\n")
            f.write(f"Normalized RMSE (NRMSE): {values['NRMSE']:.6f}\n\n")
            
            print("\nüìä Simulation Metrics:")
        for metric_name, values in metrics.items():
            print(f"\n=== {metric_name} ===")
            print("\n".join(f"{key}: {value:.6f}" for key, value in values.items()))
        

    print(f"‚úÖ Simulation metrics saved at: {metrics_file}")

    # ‚úÖ Save dataset with simulated vs. true values
    for metric_name in ["Acceleration", "Speed", "Position"]:
        df_simulated_trimmed[f"{metric_name}_True"] = df_test_trimmed[true_columns[metric_name]]

    combined_file = os.path.join(results_folder, f"simulated_vs_true_{model_type}.csv")
    df_simulated_trimmed.to_csv(combined_file, index=False)
    print(f"‚úÖ Simulated results saved at: {combined_file}")

    return df_test_trimmed, df_simulated_trimmed, metrics_file


# ‚úÖ Main Execution
def main():
    
    model_type = "Transformer"
    input_size=5
    results_folder = fr"C:\Users\renanfavero\OneDrive - University of Florida\My_research\Lake_Nona_2022\code_paper2\results_{model_type}"

    #model, scaler_x, scaler_y = load_model_and_scalers(model_type, results_folder) # I needed to run the file LSTM9 to obtain the model and scalers, not sure why load didnt worked

    # ‚úÖ Load Transformer model and scaling parameters
    #model, X_mean, X_std, y_mean, y_std = load_transformer_model(results_folder) # same error than the LSTM, the model was not able to run if loaded, so I need first run the Transformer_10 file, and later manually run inside the main

    test_file_name = r"C:\Users\renanfavero\OneDrive - University of Florida\My_research\Lake_Nona_2022\CSV_2\dataset_test_simulation.csv"
    variables_used = ['delta_s', 'delta_v', 'Follower_sp_ft', 'Follower_sp_[ft]', 'Follower_acc_t-1']

    # Load test dataset
    df_test, x_test, y_test = load_test_data(test_file_name, variables_used)

    # Make predictions
    #y_pred_unscaled, x_test_scaled, y_test_scaled = make_predictions(model, x_test, y_test, scaler_x, scaler_y, model_type)
    
    y_pred_unscaled, x_test_scaled_seq, y_test_scaled = make_transformer_predictions(model, x_test, y_test, X_mean, X_std, y_mean, y_std, input_size)

    # Save Predictions
    save_predictions(df_test, y_pred_unscaled, results_folder, model_type)
    
    # Evaluate model and save a new DF
    df_test_trimmed, y_pred_trimmed, combined_file = evaluate_model(df_test, y_pred_unscaled, results_folder, model_type,input_size)
    
    #Simulate vehicle motion using the Transformer model
    df_simulated = simulate_vehicle_motion(
        df_test, model, results_folder, X_mean, X_std, y_mean, y_std, model_type, x_test_scaled_seq, y_test_scaled, input_size)    
    
    # Simulate 
    #df_simulated = simulate_vehicle_motion(df_test, model, results_folder, scaler_x, scaler_y, model_type, x_test_scaled, y_test_scaled, input_size, horizon=1)



    # Plot, save and visualize predictions results
    plot_results(df_test_trimmed, y_pred_trimmed, results_folder, model_type, input_size)

    # Plot, save and visualize simulation results
    plot_simulation_results(df_simulated, df_test, results_folder, model_type)
    
    # Calculate simulation erros and save a new DF
    df_test_trimmed, df_simulated_trimmed, metrics_file = evaluate_simulation(df_simulated, df_test, results_folder, model_type, input_size)


    

    print(f"‚úÖ All test results saved in: {results_folder}")


# ‚úÖ Run Script
if __name__ == "__main__":
    main()
